import os
import streamlit as st
import time
import base64
import tempfile
from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_upstage import ChatUpstage
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import chromadb

chromadb.api.client.SharedSystemClient.clear_system_cache()

os.environ["UPSTAGE_API_KEY"] = "up_CAb1wgEBbFqTiFuGvb1EhXakso2jr"
if not os.getenv("UPSTAGE_API_KEY"):
    os.environ["UPSTAGE_API_KEY"] = st.text_input("Enter your UPSTAGE API KEY:", type="password")
    if not os.environ["UPSTAGE_API_KEY"]:
        st.stop()

if "id" not in st.session_state:
    st.session_state.id = "CHAT2"
    st.session_state.file_cache = {}

session_id = st.session_state.id


def reset_chat():
    st.session_state[f"{session_id}_messages"] = []
    st.session_state.context = None
    st.session_state["summary_done"] = False


def display_pdf(file):
    st.markdown("### PDF Preview")
    base64_pdf = base64.b64encode(file.read()).decode("utf-8")
    pdf_display = f"""<iframe src="data:application/pdf;base64,{base64_pdf}" width="400" height="100%" type="application/pdf"
                        style="height:50vh; width:100%"
                    >
                    </iframe>"""
    st.markdown(pdf_display, unsafe_allow_html=True)


def process_pdf(uploaded_file, max_pages=20):
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            file_path = os.path.join(temp_dir, uploaded_file.name)

            with open(file_path, "wb") as f:
                f.write(uploaded_file.getvalue())

            loader = PyPDFLoader(file_path)
            pages = loader.load_and_split()
            if len(pages) > max_pages:
                pages = pages[:max_pages]

            vectorstore = Chroma.from_documents(pages, UpstageEmbeddings(model="solar-embedding-1-large"))
            retriever = vectorstore.as_retriever(k=2)
            return retriever
    except Exception as e:
        st.error(f"Error processing PDF: {e}")
        return None

# CSSë¡œ ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜ í´ë¦­ ë§‰ê¸°
st.markdown(
    """
    <style>
    //[data-testid="stSidebarNav"] {
        //display: none;
        //pointer-events: none;  /* í´ë¦­ ì´ë²¤íŠ¸ ë¹„í™œì„±í™” */
        //opacity: 0.5;          /* ë©”ë‰´ íë¦¬ê²Œ (ì„ íƒ ì‚¬í•­) */
    //}
        /* Home ë§í¬ ë¹„í™œì„±í™” */
    [data-testid="stSidebarNav"] a[href*="home"] {
        pointer-events: none; /* í´ë¦­ ë°©ì§€ */
        //color: gray;          /* í…ìŠ¤íŠ¸ ìƒ‰ìƒ ë³€ê²½ (ì„ íƒ ì‚¬í•­) */
        opacity: 0.8; 
        text-decoration: none; 
        background-color: transparent;
    }
    </style> 
    """,
    unsafe_allow_html=True,
)

st.title("ğŸ“‘ PDF ìš”ì•½ Chatbot")
st.markdown("### ğŸ“Œ **ì‚¬ìš© ë°©ë²•**")
st.markdown("""
1. **PDF ì—…ë¡œë“œ**: í•˜ë‹¨ì˜ ì—…ë¡œë“œ ì°½ì„ í†µí•´ PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.
2. **ë¬¸ì„œ ìš”ì•½**: ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.
3. **ì§ˆë¬¸í•˜ê¸°**: ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
""")
st.markdown("---")
uploaded_file = st.file_uploader("Choose your `.pdf` file", type="pdf")

if uploaded_file:
    if uploaded_file.name not in st.session_state.file_cache:
        reset_chat()
    st.session_state.file_cache[uploaded_file.name] = uploaded_file
    retriever = process_pdf(uploaded_file)
    if retriever:
        chat = ChatUpstage(upstage_api_key=os.getenv("UPSTAGE_API_KEY"), model="solar-pro")

        contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”.
        ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""

        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", contextualize_q_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        history_aware_retriever = create_history_aware_retriever(
            chat, retriever, contextualize_q_prompt
        )

        qa_system_prompt = """ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤.
        ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
        ë‹µë³€ì€ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.

        ## ë‹µë³€ ì˜ˆì‹œ
        ğŸ“ë‹µë³€ ë‚´ìš©:
        {context}"""

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", qa_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

        st.success("Ready to Chat!")
        display_pdf(uploaded_file)

        if "summary_done" not in st.session_state or not st.session_state["summary_done"]:
            with st.spinner("ë¬¸ì„œ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                summary_prompt = f"ì´ ë¬¸ì„œì— ëŒ€í•œ ìš”ì•½ì„ 3ì¤„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”: {uploaded_file.name}"
                result_summary = rag_chain.invoke(
                    {"input": summary_prompt, "chat_history": st.session_state.get(f"{session_id}_messages", [])}
                )

            st.session_state[f"{session_id}_messages"] = [{"role": "assistant", "content": result_summary["answer"]}]
            st.session_state["summary_done"] = True

if f"{session_id}_messages" not in st.session_state:
    st.session_state[f"{session_id}_messages"] = []

for message in st.session_state.get(f"{session_id}_messages", []):
    if isinstance(message, dict):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

MAX_MESSAGES_BEFORE_DELETION = 4

if prompt := st.chat_input("Ask a question!"):
    if len(st.session_state.get(f"{session_id}_messages", [])) >= MAX_MESSAGES_BEFORE_DELETION:
        del st.session_state[f"{session_id}_messages"][:2]

    st.session_state[f"{session_id}_messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        result = rag_chain.invoke({"input": prompt, "chat_history": st.session_state.get(f"{session_id}_messages", [])})

        for chunk in result["answer"].split(" "):
            full_response += chunk + " "
            time.sleep(0.2)
            message_placeholder.markdown(full_response + "\u2588")
            message_placeholder.markdown(full_response)

    st.session_state[f"{session_id}_messages"].append({"role": "assistant", "content": full_response})

print("_______________________")
print(st.session_state.get(f"{session_id}_messages", []))
